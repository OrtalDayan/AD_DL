{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "SkipTest",
     "evalue": "You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSkipTest\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-613d3ba68cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv3DDNNLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool3DDNNLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/lasagne/layers/dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgpu_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the folder will be skipped by nosetests without failing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m raise SkipTest(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"You are importing theano.sandbox.cuda. This is the old GPU back-end and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"is removed from Theano. Use Theano 0.9 to use it. Even better, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"transition to the new GPU back-end! See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSkipTest\u001b[0m: You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import lasagne\n",
    "import theano\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers.dnn import Conv3DDNNLayer\n",
    "from lasagne.layers.dnn import Pool3DDNNLayer\n",
    "from lasagne.layers import BatchNormLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.nonlinearities import identity, softmax\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch iteration functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, iterate_minibatches_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor5(name='input', dtype='float32')\n",
    "target_var = T.ivector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    \"\"\"Method for VoxResNet Building.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "        Network dictionary.\n",
    "    \"\"\"\n",
    "    net = {}\n",
    "    net['input'] = InputLayer((None, 1, 110, 110, 110), input_var=input_var)\n",
    "    net['conv1a'] = Conv3DDNNLayer(net['input'], 32, 3, pad='same',\n",
    "                                   nonlinearity=identity)\n",
    "    net['bn1a'] = BatchNormLayer(net['conv1a'])\n",
    "    net['relu1a'] = NonlinearityLayer(net['bn1a'])\n",
    "    net['conv1b'] = Conv3DDNNLayer(net['relu1a'], 32, 3, pad='same',\n",
    "                                   nonlinearity=identity)\n",
    "    net['bn1b'] = BatchNormLayer(net['conv1b'])\n",
    "    net['relu1b'] = NonlinearityLayer(net['bn1b'])\n",
    "    net['conv1c'] = Conv3DDNNLayer(net['relu1b'], 64, 3, stride=(2, 2, 2),\n",
    "                                   pad='same', nonlinearity=identity)\n",
    "    # VoxRes block 2\n",
    "    net['voxres2_bn1'] = BatchNormLayer(net['conv1c'])\n",
    "    net['voxres2_relu1'] = NonlinearityLayer(net['voxres2_bn1'])\n",
    "    net['voxres2_conv1'] = Conv3DDNNLayer(net['voxres2_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres2_bn2'] = BatchNormLayer(net['voxres2_conv1'])\n",
    "    net['voxres2_relu2'] = NonlinearityLayer(net['voxres2_bn2'])\n",
    "    net['voxres2_conv2'] = Conv3DDNNLayer(net['voxres2_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres2_out'] = ElemwiseSumLayer([net['conv1c'],\n",
    "                                           net['voxres2_conv2']])\n",
    "    # VoxRes block 3\n",
    "    net['voxres3_bn1'] = BatchNormLayer(net['voxres2_out'])\n",
    "    net['voxres3_relu1'] = NonlinearityLayer(net['voxres3_bn1'])\n",
    "    net['voxres3_conv1'] = Conv3DDNNLayer(net['voxres3_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres3_bn2'] = BatchNormLayer(net['voxres3_conv1'])\n",
    "    net['voxres3_relu2'] = NonlinearityLayer(net['voxres3_bn2'])\n",
    "    net['voxres3_conv2'] = Conv3DDNNLayer(net['voxres3_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres3_out'] = ElemwiseSumLayer([net['voxres2_out'],\n",
    "                                           net['voxres3_conv2']])\n",
    "\n",
    "    net['bn4'] = BatchNormLayer(net['voxres3_out'])\n",
    "    net['relu4'] = NonlinearityLayer(net['bn4'])\n",
    "    net['conv4'] = Conv3DDNNLayer(net['relu4'], 64, 3, stride=(2, 2, 2),\n",
    "                                  pad='same', nonlinearity=identity)\n",
    "    # VoxRes block 5\n",
    "    net['voxres5_bn1'] = BatchNormLayer(net['conv4'])\n",
    "    net['voxres5_relu1'] = NonlinearityLayer(net['voxres5_bn1'])\n",
    "    net['voxres5_conv1'] = Conv3DDNNLayer(net['voxres5_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres5_bn2'] = BatchNormLayer(net['voxres5_conv1'])\n",
    "    net['voxres5_relu2'] = NonlinearityLayer(net['voxres5_bn2'])\n",
    "    net['voxres5_conv2'] = Conv3DDNNLayer(net['voxres5_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres5_out'] = ElemwiseSumLayer([net['conv4'], net['voxres5_conv2']])\n",
    "    # VoxRes block 6\n",
    "    net['voxres6_bn1'] = BatchNormLayer(net['voxres5_out'])\n",
    "    net['voxres6_relu1'] = NonlinearityLayer(net['voxres6_bn1'])\n",
    "    net['voxres6_conv1'] = Conv3DDNNLayer(net['voxres6_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres6_bn2'] = BatchNormLayer(net['voxres6_conv1'])\n",
    "    net['voxres6_relu2'] = NonlinearityLayer(net['voxres6_bn2'])\n",
    "    net['voxres6_conv2'] = Conv3DDNNLayer(net['voxres6_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres6_out'] = ElemwiseSumLayer([net['voxres5_out'],\n",
    "                                           net['voxres6_conv2']])\n",
    "\n",
    "    net['bn7'] = BatchNormLayer(net['voxres6_out'])\n",
    "    net['relu7'] = NonlinearityLayer(net['bn7'])\n",
    "    net['conv7'] = Conv3DDNNLayer(net['relu7'], 128, 3, stride=(2, 2, 2),\n",
    "                                  pad='same', nonlinearity=identity)\n",
    "\n",
    "    # VoxRes block 8\n",
    "    net['voxres8_bn1'] = BatchNormLayer(net['conv7'])\n",
    "    net['voxres8_relu1'] = NonlinearityLayer(net['voxres8_bn1'])\n",
    "    net['voxres8_conv1'] = Conv3DDNNLayer(net['voxres8_relu1'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres8_bn2'] = BatchNormLayer(net['voxres8_conv1'])\n",
    "    net['voxres8_relu2'] = NonlinearityLayer(net['voxres8_bn2'])\n",
    "    net['voxres8_conv2'] = Conv3DDNNLayer(net['voxres8_relu2'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres8_out'] = ElemwiseSumLayer([net['conv7'], net['voxres8_conv2']])\n",
    "    # VoxRes block 9\n",
    "    net['voxres9_bn1'] = BatchNormLayer(net['voxres8_out'])\n",
    "    net['voxres9_relu1'] = NonlinearityLayer(net['voxres9_bn1'])\n",
    "    net['voxres9_conv1'] = Conv3DDNNLayer(net['voxres9_relu1'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres9_bn2'] = BatchNormLayer(net['voxres9_conv1'])\n",
    "    net['voxres9_relu2'] = NonlinearityLayer(net['voxres9_bn2'])\n",
    "    net['voxres9_conv2'] = Conv3DDNNLayer(net['voxres9_relu2'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres9_out'] = ElemwiseSumLayer([net['voxres8_out'],\n",
    "                                           net['voxres9_conv2']])\n",
    "\n",
    "    net['pool10'] = Pool3DDNNLayer(net['voxres9_out'], 7)\n",
    "    net['fc11'] = DenseLayer(net['pool10'], 128)\n",
    "    net['prob'] = DenseLayer(net['fc11'], 2, nonlinearity=softmax)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(format='[%(asctime)s]  %(message)s',\n",
    "                    datefmt='%d.%m %H:%M:%S',\n",
    "                    level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(first_class, second_class, results_folder,\n",
    "                 num_epochs=70, batchsize=3):\n",
    "    \"\"\"Iterate minibatches on train subset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_class : {'AD', 'LMCI', 'EMCI', 'Normal'}\n",
    "        String label for target == 0.\n",
    "    second_class : {'AD', 'LMCI', 'EMCI', 'Normal'}\n",
    "        String label for target == 1.\n",
    "    results_folder : string\n",
    "        Folder to store results.\n",
    "    num_epochs : integer\n",
    "        Number of epochs for all of the experiments. Default is 70.\n",
    "    batchsize : integer\n",
    "        Batchsize for network training. Default is 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    if first_class not in {'AD', 'LMCI', 'EMCI', 'Normal'}:\n",
    "        msg = \"First class must be 'AD', 'LMCI', 'EMCI' or 'Normal', not {0}\"\n",
    "        raise ValueError(msg.format(first_class))\n",
    "    \n",
    "    if second_class not in {'AD', 'LMCI', 'EMCI', 'Normal'}:\n",
    "        msg = \"Second class must be 'AD', 'LMCI', 'EMCI' or 'Normal', not {0}\"\n",
    "        raise ValueError(msg.format(second_class))\n",
    "        \n",
    "    if first_class == second_class:\n",
    "        raise ValueError(\"Class labels should be different\")\n",
    "        \n",
    "    if not os.path.exists(results_folder):\n",
    "        os.makedirs(results_folder)\n",
    "    \n",
    "    metadata = pd.read_csv('data/metadata.csv')\n",
    "    smc_mask = ((metadata.Label == first_class) |\n",
    "                (metadata.Label == second_class)).values.astype('bool')\n",
    "    data = np.zeros((smc_mask.sum(), 1, 110, 110, 110), dtype='float32')\n",
    "\n",
    "    for it, im in tqdm(enumerate(metadata[smc_mask].Path.values),\n",
    "                       total=smc_mask.sum(), desc='Reading MRI to memory'):\n",
    "        mx = nib.load(im).get_data().max(axis=0).max(axis=0).max(axis=0)\n",
    "        data[it, 0, :, :, :] = np.array(nib.load(im).get_data()) / mx\n",
    "\n",
    "    target = (metadata[smc_mask].Label == second_class).values.astype('int32')\n",
    "    \n",
    "    for cvrand in range(5):\n",
    "        cv = StratifiedKFold(target, n_folds=5, random_state=42 * cvrand,\n",
    "                             shuffle=True)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(cv):\n",
    "            logging.debug('Starting fold {}'.format(fold))\n",
    "            X_train, y_train = data[train_index], target[train_index]\n",
    "            X_test, y_test = data[test_index], target[test_index]\n",
    "\n",
    "            net = build_net()\n",
    "\n",
    "            prediction = lasagne.layers.get_output(net['prob'])\n",
    "            loss = lasagne.objectives.categorical_crossentropy(prediction,\n",
    "                                                               target_var)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            params = lasagne.layers.get_all_params(net['prob'], trainable=True)\n",
    "            updates = lasagne.updates.nesterov_momentum(loss, params, 0.001)\n",
    "\n",
    "            test_prediction = lasagne.layers.get_output(net['prob'],\n",
    "                                                        deterministic=True)\n",
    "            test_loss = categorical_crossentropy(test_prediction, target_var)\n",
    "            test_loss = test_loss.mean()\n",
    "\n",
    "            train_fn = theano.function([input_var, target_var], loss,\n",
    "                                       updates=updates)\n",
    "            val_fn = theano.function([input_var, target_var], test_loss)\n",
    "            test_fn = theano.function([input_var], test_prediction)\n",
    "\n",
    "            logging.debug(\"Done building net\")\n",
    "\n",
    "            eps = []\n",
    "            tr_losses = []\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            val_rocs = []\n",
    "\n",
    "            logging.debug(\"Starting training...\")\n",
    "            den = X_train.shape[0] / batchsize\n",
    "            for epoch in range(num_epochs):\n",
    "                train_err = 0\n",
    "                train_batches = 0\n",
    "                start_time = time.time()\n",
    "                t = tqdm(iterate_minibatches_train(X_train, y_train, batchsize,\n",
    "                                                   shuffle=True),\n",
    "                         total=int(den),\n",
    "                         desc='Epoch {}, Loss = inf'.format(epoch + 1))\n",
    "                for batch in t:\n",
    "                    inputs, targets = batch\n",
    "                    train_err += train_fn(inputs, targets)\n",
    "                    train_batches += 1\n",
    "                    ls = train_err / train_batches\n",
    "                    t.set_description('Epoch {}, Loss = {:.5f}'.format(epoch +\n",
    "                                                                       1, ls))\n",
    "\n",
    "                val_err = 0\n",
    "                val_batches = 0\n",
    "                preds = []\n",
    "                targ = []\n",
    "                for batch in iterate_minibatches(X_test, y_test, batchsize,\n",
    "                                                 shuffle=False):\n",
    "                    inputs, targets = batch\n",
    "                    err = val_fn(inputs, targets)\n",
    "                    val_err += err\n",
    "                    val_batches += 1\n",
    "                    out = test_fn(inputs)\n",
    "                    [preds.append(i) for i in out]\n",
    "                    [targ.append(i) for i in targets]\n",
    "\n",
    "                logging.debug(\"Epoch {} done\".format(epoch + 1))\n",
    "                print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                    epoch + 1, num_epochs, time.time() - start_time),\n",
    "                    flush=True)\n",
    "                print(\"  training loss:\\t\\t{:.7f}\".format(\n",
    "                    train_err / train_batches), flush=True)\n",
    "                print(\"  validation loss:\\t\\t{:.7f}\".format(\n",
    "                    val_err / val_batches), flush=True)\n",
    "                print(\"  validation accuracy:\\t\\t{:.7f}\".format(\n",
    "                    accuracy_score(np.array(targ),\n",
    "                                   np.array(preds).argmax(axis=1))),\n",
    "                      flush=True)\n",
    "                print(\"  validation auc:\\t\\t{:.7f}\".format(\n",
    "                    roc_auc_score(np.array(targ),\n",
    "                                  np.array(preds)[:, 1])), flush=True)\n",
    "\n",
    "                eps.append(epoch)\n",
    "                tr_losses.append(train_err / train_batches)\n",
    "                val_losses.append(val_err / val_batches)\n",
    "                val_accs.append(accuracy_score(np.array(targ),\n",
    "                                               np.array(preds).argmax(axis=1)))\n",
    "                val_rocs.append(roc_auc_score(np.array(targ),\n",
    "                                              np.array(preds)[:, 1]))\n",
    "\n",
    "\n",
    "            np.save('./{}/{}_{}nm_eps.npy'.format(results_folder, \n",
    "                                                  cvrand, fold),\n",
    "                    np.array(eps))\n",
    "            np.save('./{}/{}_{}nm_tr_loss.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(tr_losses))\n",
    "            np.save('./{}/{}_{}nm_vl_loss.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_losses))\n",
    "            np.save('./{}/{}_{}nm_vl_accs.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_accs))\n",
    "            np.save('./{}/{}_{}nm_vl_rocs.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_rocs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training('AD', 'Normal', './results_resnet/ad_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('AD', 'EMCI', './results_resnet/ad_vs_emci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('AD', 'LMCI', './results_resnet/ad_vs_lmci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('EMCI', 'Normal', './results_resnet/emci_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('LMCI', 'Normal', './results_resnet/lmci_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('LMCI', 'EMCI', './results_resnet/lmci_vs_emci')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
